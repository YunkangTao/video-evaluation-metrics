#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬-è§†é¢‘å¯¹é½è¯„ä¼°è„šæœ¬
ä½¿ç”¨ViCLIPæ¨¡å‹è¯„ä¼°è§†é¢‘ä¸å…¶å¯¹åº”promptçš„å¯¹é½ç¨‹åº¦
è§†é¢‘æ–‡ä»¶åå³ä¸ºpromptï¼ˆä¸å«.mp4åç¼€ï¼‰
"""

import os
import json
import argparse
import numpy as np
import torch
from tqdm import tqdm

from vbench.utils import clip_transform, read_frames_decord_by_fps, CACHE_DIR
from vbench.third_party.ViCLIP.viclip import ViCLIP
from vbench.third_party.ViCLIP.simple_tokenizer import SimpleTokenizer


def get_text_features(model, input_text, tokenizer, text_feature_dict={}):
    """è·å–æ–‡æœ¬ç‰¹å¾å‘é‡"""
    if input_text in text_feature_dict:
        return text_feature_dict[input_text]
    text_template = f"{input_text}"
    with torch.no_grad():
        text_features = model.encode_text(text_template).float()
        text_features /= text_features.norm(dim=-1, keepdim=True)
        text_feature_dict[input_text] = text_features
    return text_features


def get_vid_features(model, input_frames):
    """è·å–è§†é¢‘ç‰¹å¾å‘é‡"""
    with torch.no_grad():
        clip_feat = model.encode_vision(input_frames, test=True).float()
        clip_feat /= clip_feat.norm(dim=-1, keepdim=True)
    return clip_feat


def extract_prompt_from_filename(filename):
    """
    ä»æ–‡ä»¶åæå–prompt
    ä¾‹å¦‚: "a cat running in the garden.mp4" -> "a cat running in the garden"
    """
    prompt = os.path.splitext(filename)[0]
    # å¦‚æœæ–‡ä»¶åä¸­æœ‰ä¸‹åˆ’çº¿ï¼Œå¯ä»¥é€‰æ‹©æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆæ ¹æ®ä½ çš„å‘½åè§„åˆ™è°ƒæ•´ï¼‰
    # prompt = prompt.replace('_', ' ')
    return prompt


def load_viclip_model(device):
    """åŠ è½½ViCLIPæ¨¡å‹"""
    print("=" * 50)
    print("Loading ViCLIP model...")
    print("=" * 50)
    
    # Tokenizerè·¯å¾„
    tokenizer_path = os.path.join(CACHE_DIR, "ViCLIP/bpe_simple_vocab_16e6.txt.gz")
    
    # æ¨¡å‹æƒé‡è·¯å¾„
    pretrain_path = os.path.join(CACHE_DIR, "ViCLIP/ViClip-InternVid-10M-FLT.pth")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(tokenizer_path):
        raise FileNotFoundError(
            f"Tokenizer not found at {tokenizer_path}\n"
            "Please download ViCLIP model files first.\n"
            "You can use: python -c \"from vbench import VBench; VBench.download_all_resources()\""
        )
    
    if not os.path.exists(pretrain_path):
        raise FileNotFoundError(
            f"Model weights not found at {pretrain_path}\n"
            "Please download ViCLIP model files first."
        )
    
    tokenizer = SimpleTokenizer(tokenizer_path)
    
    viclip = ViCLIP(
        tokenizer=tokenizer,
        pretrain=pretrain_path
    ).to(device)
    viclip.eval()
    
    print("ViCLIP model loaded successfully!")
    return viclip, tokenizer


def evaluate_text_video_alignment(
    video_folder, 
    device, 
    num_frames=8,
    sample="middle"
):
    """
    è¯„ä¼°æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰è§†é¢‘çš„æ–‡æœ¬-è§†é¢‘å¯¹é½ç¨‹åº¦
    
    Args:
        video_folder: åŒ…å«MP4æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        num_frames: é‡‡æ ·å¸§æ•°
        sample: é‡‡æ ·ç­–ç•¥ ('middle', 'uniform')
    
    Returns:
        avg_score: å¹³å‡å¯¹é½åˆ†æ•°
        video_results: æ¯ä¸ªè§†é¢‘çš„è¯¦ç»†ç»“æœ
    """
    
    # åŠ è½½æ¨¡å‹
    viclip, tokenizer = load_viclip_model(device)
    
    # å›¾åƒå˜æ¢
    image_transform = clip_transform(224)
    
    # è·å–æ‰€æœ‰MP4æ–‡ä»¶
    video_files = sorted([
        f for f in os.listdir(video_folder) 
        if f.lower().endswith('.mp4')
    ])
    
    if len(video_files) == 0:
        print(f"Error: No MP4 files found in {video_folder}")
        return None, []
    
    print(f"\nFound {len(video_files)} video files to evaluate")
    print("-" * 50)
    
    sim_scores = []
    video_results = []
    text_feature_dict = {}  # ç¼“å­˜æ–‡æœ¬ç‰¹å¾
    failed_videos = []
    
    for video_file in tqdm(video_files, desc="Evaluating videos"):
        video_path = os.path.join(video_folder, video_file)
        prompt = extract_prompt_from_filename(video_file)
        
        try:
            with torch.no_grad():
                # è¯»å–è§†é¢‘å¸§
                images = read_frames_decord_by_fps(
                    video_path, 
                    num_frames=num_frames, 
                    sample=sample
                )
                images = image_transform(images)
                images = images.to(device)
                
                # è·å–è§†é¢‘ç‰¹å¾
                clip_feat = get_vid_features(viclip, images.unsqueeze(0))
                
                # è·å–æ–‡æœ¬ç‰¹å¾
                text_feat = get_text_features(
                    viclip, prompt, tokenizer, text_feature_dict
                )
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                logit_per_text = clip_feat @ text_feat.T
                score = float(logit_per_text[0][0].cpu())
                
                sim_scores.append(score)
                video_results.append({
                    'video_path': video_path,
                    'video_name': video_file,
                    'prompt': prompt,
                    'alignment_score': score
                })
                
        except Exception as e:
            print(f"\nError processing {video_file}: {e}")
            failed_videos.append({
                'video_name': video_file,
                'error': str(e)
            })
            continue
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if sim_scores:
        avg_score = float(np.mean(sim_scores))
        std_score = float(np.std(sim_scores))
        min_score = float(np.min(sim_scores))
        max_score = float(np.max(sim_scores))
    else:
        avg_score = std_score = min_score = max_score = 0
    
    stats = {
        'average': avg_score,
        'std': std_score,
        'min': min_score,
        'max': max_score,
        'num_evaluated': len(sim_scores),
        'num_failed': len(failed_videos)
    }
    
    return stats, video_results, failed_videos


def print_results(stats, video_results):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "=" * 60)
    print("EVALUATION RESULTS")
    print("=" * 60)
    
    print(f"\nğŸ“Š Statistics:")
    print(f"   â€¢ Videos evaluated: {stats['num_evaluated']}")
    print(f"   â€¢ Videos failed:    {stats['num_failed']}")
    print(f"   â€¢ Average score:    {stats['average']:.4f}")
    print(f"   â€¢ Std deviation:    {stats['std']:.4f}")
    print(f"   â€¢ Min score:        {stats['min']:.4f}")
    print(f"   â€¢ Max score:        {stats['max']:.4f}")
    
    if video_results:
        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(
            video_results, 
            key=lambda x: x['alignment_score'], 
            reverse=True
        )
        
        # æ˜¾ç¤ºæœ€é«˜åˆ†
        print(f"\nğŸ† Top 5 Best Aligned Videos:")
        for i, res in enumerate(sorted_results[:5], 1):
            prompt_display = res['prompt'][:60] + "..." if len(res['prompt']) > 60 else res['prompt']
            print(f"   {i}. [{res['alignment_score']:.4f}] {prompt_display}")
        
        # æ˜¾ç¤ºæœ€ä½åˆ†
        print(f"\nâš ï¸  Top 5 Worst Aligned Videos:")
        for i, res in enumerate(sorted_results[-5:], 1):
            prompt_display = res['prompt'][:60] + "..." if len(res['prompt']) > 60 else res['prompt']
            print(f"   {i}. [{res['alignment_score']:.4f}] {prompt_display}")
    
    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description='Evaluate text-video alignment using ViCLIP (VBench)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python evaluate_alignment.py --video_folder ./generated_videos
  python evaluate_alignment.py --video_folder ./videos --output results.json --device cuda:0
  python evaluate_alignment.py --video_folder ./videos --num_frames 16 --sample uniform
        """
    )
    
    parser.add_argument(
        '--video_folder', 
        type=str, 
        required=True,
        help='Path to folder containing MP4 files (filename = prompt)'
    )
    parser.add_argument(
        '--output', 
        type=str, 
        default='alignment_results.json',
        help='Output JSON file path (default: alignment_results.json)'
    )
    parser.add_argument(
        '--device', 
        type=str, 
        default='cuda',
        help='Device to use: cuda, cuda:0, cuda:1, or cpu (default: cuda)'
    )
    parser.add_argument(
        '--num_frames', 
        type=int, 
        default=8,
        help='Number of frames to sample from each video (default: 8)'
    )
    parser.add_argument(
        '--sample', 
        type=str, 
        default='middle',
        choices=['middle', 'uniform'],
        help='Frame sampling strategy (default: middle)'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤¹
    if not os.path.isdir(args.video_folder):
        print(f"Error: Video folder not found: {args.video_folder}")
        return
    
    # æ£€æŸ¥è®¾å¤‡
    if 'cuda' in args.device and not torch.cuda.is_available():
        print("Warning: CUDA not available, using CPU instead")
        device = torch.device('cpu')
    else:
        device = torch.device(args.device)
    
    print(f"Using device: {device}")
    
    # æ‰§è¡Œè¯„ä¼°
    stats, video_results, failed_videos = evaluate_text_video_alignment(
        video_folder=args.video_folder,
        device=device,
        num_frames=args.num_frames,
        sample=args.sample
    )
    
    if stats is None:
        return
    
    # æ‰“å°ç»“æœ
    print_results(stats, video_results)
    
    # ä¿å­˜ç»“æœåˆ°JSON
    output_data = {
        'config': {
            'video_folder': os.path.abspath(args.video_folder),
            'num_frames': args.num_frames,
            'sample_strategy': args.sample,
            'device': str(device)
        },
        'statistics': stats,
        'video_results': video_results,
        'failed_videos': failed_videos
    }
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ Results saved to: {args.output}")


if __name__ == '__main__':
    main()